{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 478,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.builder:Found cached dataset wikitext (/Users/jdvin/.cache/huggingface/datasets/wikitext/wikitext-2-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9e5472758f7466fb89bb0f62a6d906b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading weights from pretrained gpt: gpt2\n",
      "forcing vocab_size=50257, block_size=1024, bias=True\n",
      "overriding dropout rate to 0\n",
      "number of parameters: 123.65M\n"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "\n",
    "from datasets import load_dataset\n",
    "import tiktoken\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "from nanoGPT import model\n",
    "import utils as ut\n",
    "\n",
    "wikitext = load_dataset(\"wikitext\", \"wikitext-2-v1\")\n",
    "\n",
    "hf_gpt_model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
    "hf_gpt_tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "nanoGPT_model = model.GPT.from_pretrained(\"gpt2\", dict(dropout=0))\n",
    "nanoGPT_model.eval()\n",
    "tiktoken_tokenizer = tiktoken.get_encoding(\"gpt2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 480,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Beginning Model Testing ===\n",
      "=> Testing model HF Base (1/5).\n",
      "==> Running speed test...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ace7cd6dacc74b73a6d81cd01b0d193b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Complete!\n",
      "==> Running perplexity test...\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'utils' has no attribute 'analyse_lm_performance'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[480], line 93\u001b[0m\n\u001b[1;32m     91\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m==> Complete!\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     92\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m==> Running perplexity test...\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> 93\u001b[0m     ut\u001b[39m.\u001b[39;49manalyse_lm_performance(test_sequences, hf_gpt_tokenizer, model_tester)\n\u001b[1;32m     94\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m==> Complete!\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     96\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m== Model Testing Complete! - Plotting results... ==\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'utils' has no attribute 'analyse_lm_performance'"
     ]
    }
   ],
   "source": [
    "importlib.reload(ut)\n",
    "\n",
    "base_hf_tester = ut.ModelTester(\n",
    "    model_name=\"HF Base\",\n",
    "    model_func=ut.hf_lm_pipe(tokenizer=hf_gpt_tokenizer, model=hf_gpt_model),\n",
    ")\n",
    "\n",
    "# nanoGPT_tester = ut.ModelTester(\n",
    "#     model_name=\"nanoGPT\",\n",
    "#     model_func=ut.nanoGPT_pipe(tiktoken_tokenizer, nanoGPT_model),\n",
    "# )\n",
    "\n",
    "\n",
    "onnx_base_tester = ut.ModelTester(\n",
    "    model_name=\"Onnx Base\",\n",
    "    model_func=ut.onnx_custom_pipe(\n",
    "        hf_gpt_tokenizer,\n",
    "        model_path=\"models/gpt2_base.onnx\",\n",
    "        n_attention_heads=hf_gpt_model.config.n_head,\n",
    "        hidden_size=hf_gpt_model.config.n_embd,\n",
    "        n_layers=hf_gpt_model.config.n_layer,\n",
    "    ),\n",
    ")\n",
    "onnx_quant_tester = ut.ModelTester(\n",
    "    model_name=\"Onnx Quant\",\n",
    "    model_func=ut.onnx_custom_pipe(\n",
    "        hf_gpt_tokenizer,\n",
    "        model_path=\"models/gpt2_quant.onnx\",\n",
    "        n_attention_heads=hf_gpt_model.config.n_head,\n",
    "        hidden_size=hf_gpt_model.config.n_embd,\n",
    "        n_layers=hf_gpt_model.config.n_layer,\n",
    "    ),\n",
    ")\n",
    "\n",
    "onnx_opt_tester = ut.ModelTester(\n",
    "    model_name=\"Onnx Opt\",\n",
    "    model_func=ut.onnx_custom_pipe(\n",
    "        hf_gpt_tokenizer,\n",
    "        model_path=\"models/gpt2_opt.onnx\",\n",
    "        n_attention_heads=hf_gpt_model.config.n_head,\n",
    "        hidden_size=hf_gpt_model.config.n_embd,\n",
    "        n_layers=hf_gpt_model.config.n_layer,\n",
    "    ),\n",
    ")\n",
    "onnx_opt_quant_tester = ut.ModelTester(\n",
    "    model_name=\"Onnx Opt Quant\",\n",
    "    model_func=ut.onnx_custom_pipe(\n",
    "        hf_gpt_tokenizer,\n",
    "        model_path=\"models/gpt2_opt_quant.onnx\",\n",
    "        n_attention_heads=hf_gpt_model.config.n_head,\n",
    "        hidden_size=hf_gpt_model.config.n_embd,\n",
    "        n_layers=hf_gpt_model.config.n_layer,\n",
    "    ),\n",
    ")\n",
    "\n",
    "testers = [\n",
    "    # nanoGPT_tester,\n",
    "    base_hf_tester,\n",
    "    # compiled_hf_tester,\n",
    "    # compiled_nanoGPT_tester,\n",
    "    onnx_base_tester,\n",
    "    onnx_quant_tester,\n",
    "    onnx_opt_tester,\n",
    "    onnx_opt_quant_tester,\n",
    "]\n",
    "\n",
    "tests_per_model = 20\n",
    "prompt_length = 10\n",
    "test_sequences = ut.sample_dataset(\n",
    "    dataset=wikitext, n_articles=10, min_length_chars=500, max_length_chars=1000\n",
    ")\n",
    "standard_sequences = [\n",
    "    \"The Wright brothers invented and flew the first airplane in 1903, recognized as 'the first sustained and controlled heavier-than-air powered flight'. They built on the works of George Cayley dating from 1799, when he set forth the concept of the modern airplane (and later built and flew models and successful passenger-carrying gliders). Between 1867 and 1896, the German pioneer of human aviation Otto Lilienthal also studied heavier-than-air flight. Following its limited use in World War I, aircraft technology continued to develop. Airplanes had a presence in all the major battles of World War II. The first jet aircraft was the German Heinkel He 178 in 1939. The first jet airliner, the de Havilland Comet, was introduced in 1952. The Boeing 707, the first widely successful commercial jet, was in commercial service for more than 50 years, from 1958 to at least 2013.\",\n",
    "    \"Telecommunications is about transferring information from one location to another. This includes many forms of information: telephone conversations, television signals, computer files, and other types of data. To transfer the information, you need a channel between the two locations. This may be a wire pair, radio signal, optical fiber, etc. Telecommunications companies receive payment for transferring their customer's information, while they must pay to establish and maintain the channel. The financial bottom line is simple: the more information they can pass through a single channel, the more money they make. DSP has revolutionized the telecommunications industry in many areas: signaling tone generation and detection, frequency band shifting, filtering to remove power line hum, etc. Three specific examples from the telephone network will be discussed here: multiplexing, compression, and echo control.\",\n",
    "]\n",
    "max_generation_length = 20\n",
    "\n",
    "\n",
    "# random.shuffle(testers)\n",
    "print(\"=== Beginning Model Testing ===\")\n",
    "for i, model_tester in enumerate(testers):\n",
    "    print(f\"=> Testing model {model_tester.model_name} ({i+1}/{len(testers)}).\")\n",
    "    print(f\"==> Running speed test...\")\n",
    "    # Conduct multiple tests to account for variance resulting from machine-level sources.\n",
    "    model_tester.run_speed_test(\n",
    "        n_tests=tests_per_model,\n",
    "        prompt_length=prompt_length,\n",
    "        max_generation_length=max_generation_length,\n",
    "        tokenizer=hf_gpt_tokenizer,\n",
    "    )\n",
    "    print(f\"==> Complete!\")\n",
    "    print(f\"==> Running perplexity test...\")\n",
    "    model_tester.run_perplexity_test(test_sequences, hf_gpt_tokenizer)\n",
    "    print(f\"==> Complete!\")\n",
    "\n",
    "print(\"== Model Testing Complete! - Plotting results... ==\")\n",
    "ut.plot_test_results(model_testers=testers, result_type=\"times\")\n",
    "ut.plot_test_results(model_testers=testers, result_type=\"perplexities\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "testers[0].outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"straight from: https://onnxruntime.ai/docs/performance/quantization.html\"\"\"\n",
    "\n",
    "from onnxruntime.quantization import quantize_dynamic, QuantType\n",
    "\n",
    "model_fp32 = \"models/gpt2_opt.onnx\"\n",
    "model_quant = \"models/gpt2_opt_quant.onnx\"\n",
    "quantized_model = quantize_dynamic(model_fp32, model_quant)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:onnx_model_bert:BertOptimizationOptions is depreciated. Please use FusionOptions instead.\n",
      "WARNING:symbolic_shape_infer:Cannot determine if -seq_len + total_seq_len < 0\n",
      "WARNING:symbolic_shape_infer:Cannot determine if -seq_len + total_seq_len < 0\n",
      "WARNING:symbolic_shape_infer:Cannot determine if -seq_len + total_seq_len < 0\n",
      "WARNING:symbolic_shape_infer:Cannot determine if -seq_len + total_seq_len < 0\n",
      "WARNING:symbolic_shape_infer:Cannot determine if -seq_len + total_seq_len < 0\n",
      "WARNING:symbolic_shape_infer:Cannot determine if -seq_len + total_seq_len < 0\n",
      "WARNING:symbolic_shape_infer:Cannot determine if -seq_len + total_seq_len < 0\n",
      "WARNING:symbolic_shape_infer:Cannot determine if -seq_len + total_seq_len < 0\n",
      "WARNING:symbolic_shape_infer:Cannot determine if -seq_len + total_seq_len < 0\n",
      "WARNING:symbolic_shape_infer:Cannot determine if -seq_len + total_seq_len < 0\n",
      "WARNING:symbolic_shape_infer:Cannot determine if -seq_len + total_seq_len < 0\n",
      "WARNING:symbolic_shape_infer:Cannot determine if -seq_len + total_seq_len < 0\n",
      "WARNING:symbolic_shape_infer:Cannot determine if -seq_len + total_seq_len < 0\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Trying pre-quantisation optimisation as per: https://github.com/microsoft/onnxruntime-inference-examples/blob/main/quantization/notebooks/bert/Bert-GLUE_OnnxRuntime_quantization.ipynb\"\"\"\n",
    "\n",
    "from onnxruntime_tools import optimizer\n",
    "\n",
    "opt_model = optimizer.optimize_model(\n",
    "    \"models/gpt2_base.onnx\",\n",
    "    \"gpt2\",\n",
    "    num_heads=hf_gpt_model.config.n_head,\n",
    "    hidden_size=hf_gpt_model.config.n_embd,\n",
    ")\n",
    "# optimization_options=opt_options)\n",
    "opt_model.save_model_to_file(\"models/gpt2_opt.onnx\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d745037ffe4aeb9469e549277da22aeab1a98e37d56e79cfbc529ff79a3fa7a8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
